# PERFIL UPWORK — Copy-paste directo (v2 con ajustes del Lead Architect)

---

## TITULO (max 70 caracteres)

```
Computational Civil Engineer | Python Automation, Data ETL & Dashboards
```

---

## OVERVIEW / DESCRIPCION PROFESIONAL

```
I am a Computational Civil Engineer bridging the gap between heavy engineering and modern Data Science. I don't just write scripts; I build robust, automated pipelines that turn messy data into actionable intelligence.

If you are losing hours doing manual data entry, wrangling with massive CSVs, or trying to make sense of complex engineering data, I can automate that pain away.

What I deliver:
- Excel/CSV automation — consolidate hundreds of files into clean reports in minutes
- Web scraping — extract structured data from any website, delivered as CSV/Excel/database
- Interactive dashboards — Plotly Dash / Streamlit visualizations from your raw data
- PDF table extraction — pull tables from engineering reports into structured spreadsheets
- API integrations — connect your systems, automate repetitive data transfers
- CFD/SPH simulation — GPU-accelerated fluid dynamics for engineering analysis

My stack: Python, Pandas, NumPy, Matplotlib, Plotly, SQLite, BeautifulSoup, Selenium, scikit-learn, lxml.

I've built a complete GPU simulation pipeline that processes terabytes of output into clean databases and publication-quality figures — automatically. I bring that same "automate everything" mindset to every project.

Fast delivery. Clean code. Clear communication.
```

---

## TARIFA

```
$25/hr (display rate — subir a $40+ despues de 3 reviews de 5 estrellas)
```

**REGLA DEL ARQUITECTO:** Nunca vendas tu hora. Vende el producto.
- Script de 50 Excels: $150 fixed price (te toma 1h con IA = $150/hr real)
- Scraper completo: $200-400 fixed price
- Dashboard: $300-500 fixed price
- Siempre cotiza FIXED PRICE, no hourly

---

## SKILLS / TAGS (seleccionar en Upwork)

```
Python
Data Processing
Web Scraping
Pandas
Data Analysis
Automation
Excel
Data Visualization
ETL Pipeline
API Integration
SQL
Dashboard Development
PDF Parsing
Technical Writing
```

---

## CATEGORIA PRINCIPAL

```
Data Science & Analytics > Data Processing
```

Subcategoria alternativa:
```
Engineering & Architecture > Other Engineering
```

---

## PORTAFOLIO — 4 piezas (subir las imagenes)

### Pieza 1: "GPU Mesh Convergence Study"
Imagen: fig09_complete_story.png
```
Title: Mesh Convergence Study — GPU-Accelerated SPH Simulation
Description: Automated convergence analysis across 6 particle resolutions on RTX 5090. Pipeline processes raw simulation output (CSV), computes metrics, and generates publication-quality figures. Built with Python, Pandas, Matplotlib.
```

### Pieza 2: "Gaussian Process Surrogate Model"
Imagen: gp_frontera_decision.png (de data/figuras_ml_test/)
```
Title: Machine Learning Surrogate — Gaussian Process with Uncertainty
Description: Probabilistic failure prediction using Gaussian Process regression. Contour map shows probability of failure with decision boundary. Trained on engineering simulation data. Python, scikit-learn, Matplotlib.
```

### Pieza 3: "Displacement Convergence Analysis"
Imagen: fig01_displacement_convergence.png
```
Title: Data Analysis & Visualization — Engineering Metrics
Description: Convergence analysis of boulder displacement across 6 mesh resolutions. Automated annotation of relative changes, convergence zone identification. Clean, publication-ready output from raw CSV data.
```

### Pieza 4: "Interactive Engineering Dashboard"
Screenshot del Plotly Dash (app.py corriendo)
```
Title: Interactive Engineering Dashboard — Plotly Dash
Description: Real-time dashboard connected to SQLite database. Scatter plots, bar charts, metric cards, and data tables with dark theme. Filters and callbacks for interactive exploration. Python, Plotly Dash, Bootstrap.
```

---

## PROPUESTAS TIPO — Formula de los 3 Segundos

**REGLA:** Las primeras 2 lineas determinan si te leen o te borran.
Siempre: (1) menciona SU problema especifico, (2) di que ya hiciste algo similar.

### Para proyectos de Excel/CSV automation:
```
Hi [Name],

I just read your request about [ej: consolidating 200 Excel reports into a single database]. I built a similar pipeline last month for an engineering firm — it processed 500+ CSVs into a clean SQLite database with automated validation.

Here is exactly how I will solve your problem:
1. Parse all your files with pandas (handling different formats/encodings)
2. Clean, validate, and merge into a single structured output
3. Generate a summary report with key metrics
4. Deliver the script + documentation so you can re-run it anytime

Deliverable: Working Python script + clean output file.
Timeline: 2-3 days.
Price: $[X] fixed.

Happy to jump on a quick call if you want to discuss specifics.

Kevin
```

### Para proyectos de web scraping:
```
Hi [Name],

I just read your post about [ej: extracting product prices from 5 competitor websites]. I recently built a scraper for a similar ecommerce project using BeautifulSoup + Selenium that handles pagination and dynamic JS content.

Here is my approach:
1. Analyze each target site's structure (static vs JS-rendered)
2. Build scrapers with retry logic and rate limiting (no IP bans)
3. Output: clean CSV/Excel with all requested fields
4. Optional: schedule it to run daily/weekly with cron

Deliverable: Working scraper + clean dataset + brief docs.
Timeline: 3-5 days depending on site complexity.
Price: $[X] fixed.

Let me know if you'd like to see a sample output first.

Kevin
```

### Para proyectos de dashboards:
```
Hi [Name],

I just read your request for [ej: a dashboard to visualize your sales data across 3 regions]. I built an interactive Plotly Dash dashboard last week for an engineering client — dark theme, filters, real-time data from SQLite.

My plan:
1. Connect to your data source (Excel/CSV/database/API)
2. Build interactive charts with filters and drill-down
3. Professional styling with cards, metrics, and responsive layout
4. Deploy as a shareable web app (or standalone script)

Deliverable: Working dashboard + source code + deployment guide.
Timeline: 4-6 days.
Price: $[X] fixed.

I can share a screenshot of a recent dashboard if you'd like to see my style.

Kevin
```

### Para proyectos de PDF extraction:
```
Hi [Name],

I just saw your post about [ej: extracting tables from 100 PDF invoices into Excel]. I've worked with pdfplumber and tabula-py extensively — I built a pipeline that extracts structured data from engineering reports with 95%+ accuracy.

My approach:
1. Analyze your PDF structure (text-based vs scanned)
2. Extract tables using pdfplumber (or OCR with Tesseract if scanned)
3. Clean and validate the extracted data
4. Output: structured Excel/CSV with all fields mapped correctly

Deliverable: Python script + clean Excel output.
Timeline: 2-4 days.
Price: $[X] fixed.

Happy to do a free test run on 2-3 sample PDFs so you can see the quality before committing.

Kevin
```

---

## CHECKLIST PARA CREAR EL PERFIL

- [ ] Foto profesional (no selfie, fondo limpio, camisa, cara visible)
- [ ] Verificar identidad (Upwork lo pide)
- [ ] Conectar GitHub: github.com/kcortes765
- [ ] Subir 4 piezas de portafolio (las figuras de arriba)
- [ ] Education: Universidad Catolica del Norte - Civil Engineering (Expected 2026)
- [ ] Employment: "Computational Research Assistant — UCN (2025 – Present)"
- [ ] Availability: Part-time (20 hrs/week)
- [ ] English proficiency: Professional
- [ ] Hacer el test de Python de Upwork (sube visibilidad)
- [ ] Postular a 10-15 proyectos la primera semana (FIXED PRICE)

**NUNCA poner "Student" en ningun lado visible del perfil.**
En Education se pone "Expected 2026" y listo. El titulo es tu codigo, no tu carton.

---

## PROYECTOS A BUSCAR (keywords en Upwork)

Tier 1 — Alta probabilidad de ganar (empezar aqui):
- "python script", "csv processing", "excel automation"
- "web scraping", "data extraction", "scrape website"
- "pandas", "data cleaning", "etl"
- "pdf to excel", "pdf extraction", "table extraction"

Tier 2 — Pagan mas:
- "dashboard", "streamlit", "plotly", "data visualization"
- "api integration", "automate workflow"
- "database", "sqlite", "data pipeline"

Tier 3 — Tu nicho premium (despues de 5+ reviews):
- "cfd simulation", "fea analysis", "engineering simulation"
- "scientific computing", "numerical analysis"
- "bim automation", "revit api"

---

## ERRORES A EVITAR

1. No cobrar menos de $15/hr — te posiciona como spam
2. No postular con mensajes genericos — SIEMPRE menciona SU problema en la linea 1
3. No aceptar proyectos sin scope claro — pide requisitos antes de aceptar
4. No entregar sin documentacion minima — un README de 5 lineas marca la diferencia
5. No ignorar mensajes — responder en <2 horas sube tu ranking
6. NUNCA decir que eres estudiante — eres Computational Research Assistant
7. SIEMPRE cotizar Fixed Price — tu tarifa real es (precio / horas con IA)
8. Ofrecer "free test run" en PDFs/scraping — convierte como loco

---

## PRICING GUIDE (lo que cobras vs lo que te toma)

| Proyecto tipo | Precio al cliente | Tu tiempo real (con IA) | Tarifa real |
|---|---|---|---|
| Unir 50 Excels | $150 | 1-2h | $75-150/hr |
| Scraper basico | $200 | 2-3h | $65-100/hr |
| Scraper complejo (JS) | $400 | 4-6h | $65-100/hr |
| PDF extraction (100 docs) | $250 | 2-3h | $80-125/hr |
| Dashboard Plotly | $400 | 4-6h | $65-100/hr |
| API integration | $300 | 3-4h | $75-100/hr |

La IA es tu apalancamiento. Cobra por el valor, no por la hora.
